# -*- coding: utf-8 -*-
"""ANN&CONLL_transformation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WBYfsUDAv4EGJdlIt1obOBQDZyRXlzJI

# ANN Transformation

Read ANN File
"""
import sys

# annfilename = "int119_cleaned_pretransformed.ann"
# txtfilename = "int119_cleaned.txt"


if __name__ == "__main__":
    txtfilename = sys.argv[1]
    annfilename = sys.argv[2]

    # Read in original brat ann file
    file1 = open(annfilename, 'r')

    Lines = file1.readlines()

    # Split lines to two groups for MENTIONS and COREF
    count = 0
    Token = []
    Relation = []

    for line in Lines:
        l = line.strip()
        if l[0] == 'T':
            Token.append(l.split('\t'))
        else:
            Relation.append(l.split('\t'))

    """MENTIONS"""

    # Split Token's second term
    for t in Token:
        t[1] = t[1].split()

    file2 = open(txtfilename, 'r')
    filename = file2.name[:-4]
    Lines2 = file2.readlines()
    Lines2

    words = Lines2[0].split()
    final = []
    count = 0
    start = 0
    end=0
    for i in words:
        temp = []
        temp.append(i)
        end = start + len(i)
        temp.append(count)
        temp.append(start)
        temp.append(end)
        start = end + 1
        count += 1
        final.append(temp)
    final
    # [Word, word_count_index, start_idx, end_idx]

    # Create MENTIONS
    import spacy
    nlp = spacy.load("en_core_web_sm")
    Mentions = []
    pron = ["i", "me","us", "we", "ourselves", "our","ours","you","your","yours","yourself","he","his","him","himself","she","her","hers","herself","they","their","theirs","themselves","it","its","itself"]
    for tok in Token:
        start_idx = 0
        end_idx = 0
        result = []
        result.append('MENTION')
        result.append(tok[0])
        for k in range(len(final)):
            if str(final[k][2]) == str(tok[1][1]):
                start_idx = k
                break
        for j in range(len(final)):
            if str(final[j][3]) == str(tok[1][2]):
                end_idx = j
                break
        result.append(0)
        result.append(start_idx)
        result.append(0)
        result.append(end_idx)
        result.append(tok[-1])
        result.append(tok[1][0])
        #print(tok[-1])
        #print(len(tok[-1].split()))
        if len(tok[-1].split()) == 1:
            if str(tok[-1]).lower() in pron:
                result.append("PRON")
        else:
            doc = nlp(str(tok[-1]))
            pos = []
            for token in doc:
                pos.append(token.pos_)
            if "PROPN" in pos:
                result.append("PROP")
            else:
                result.append("NOM")


        Mentions.append(result)
    Mentions

    """COREF"""

    Relation

    # Extract coref pairs
    count = 0
    L = []
    for r in Relation:
        L.append((r[1].split()[1].split(':')[1], r[1].split()[2].split(':')[1]))
    L

    # Grouping coref
    import networkx as nx
    import itertools

    G=nx.from_edgelist(L)
    l=list(nx.connected_components(G))
    mapdict={z:x for x, y in enumerate(l) for z in y }
    newlist=[ x+(mapdict[x[0]],)for  x in L]
    newlist=sorted(newlist,key=lambda x : x[2])
    coref_idx_list=[list(y) for x , y in itertools.groupby(newlist,key=lambda x : x[2])]
    coref_idx_list

    # Create dictionary with assigned index
    coref_dict = dict()
    for i in range(len(coref_idx_list)):
        temp = []
        for j in range(len(coref_idx_list[i])):
            temp.append(coref_idx_list[i][j][0])
            temp.append(coref_idx_list[i][j][1])
        temp = list(set(temp))
        coref_dict[i]=temp

    final_corefdict = dict()
    for key, value in coref_dict.items():
        for i in value:
            final_corefdict[i] = key
    final_corefdict

    # Create COREF
    Coref = []
    for key, value in final_corefdict.items():
        temp = []
        temp.append("COREF")
        temp.append(key)
        for m in Mentions:
            if key == m[1]:
                temp.append(m[6]+'-'+str(value))
        Coref.append(temp)
    Coref

    """Write new ANN File"""

    # Transform MENTIONS type for output convenience
    Mention = []
    for m in Mentions:
        m = [str(i) for i in m]
        Mention.append(m)
    Mention

    f = open("{}.ann".format(filename), "w")
    for m in Mention:
        f.write('\t'.join(m)+"\n")
    for c in Coref:
        f.write('\t'.join(c)+"\n")
    f.close()


    """# Generate CONLL file"""

    conll_lines = []
    for i in range(len(final)):
        temp = []
        temp.append(filename)
        temp.append(0)
        temp.append(final[i][1])
        temp.append(final[i][0])
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        temp.append("_")
        conll_lines.append(temp)
    conll_lines

    Coref

    for c in Coref:
        for m in Mentions:
            if c[1] == m[1]:
                start = m[3]
                end = m[5]
                break
        if start != end:
            conll_lines[start].append("("+str(c[2].split("-")[1]))
            conll_lines[end].append(str(c[2].split("-")[1])+")")
        else:
            conll_lines[start].append("("+str(c[2].split("-")[1])+")")
    conll_lines

    # Transform type for output convenience
    Conll = []
    for c in conll_lines:
        c = [str(i) for i in c]
        Conll.append(c)
    Conll

    f = open("{}.conll".format(filename), "w")
    f.write("#begin document ({}); part 0".format(filename)+"\n")
    for c in Conll:
        f.write('\t'.join(c)+"\n")
    f.write("#end document")
    f.close()
